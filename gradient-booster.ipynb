{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# for jupyter notebook\n%matplotlib inline \nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import Imputer\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"779fce13a2f2a31f0208e1f8a99de857a50ce413"},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4aa865517a727de24cec9e8a5baae6e28b82cedd"},"cell_type":"code","source":"# save the target and ID separately\ntrain_ID = train_df['Id']\ntest_ID = test_df['Id']\ntarget_df = train_df['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf9bce2012bfb95cfe9a843c230af2e88506f20e"},"cell_type":"code","source":"# create a list of headings, remove SalePrice because target\npossible_features = train_df.columns.tolist()\npossible_features.remove('SalePrice')\npossible_features.remove('Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a39d7f6ab10492c0061713174516ececb608f556"},"cell_type":"code","source":"# save the length of each dataframe so we can combine them for \n# pre-processing and then split them later\nindex_train = train_df.shape[0]\nindex_test = test_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4580c5c096369eba4fcecc0d09749b1da6d6899"},"cell_type":"code","source":"# combine for pre-processing, drop target so we do not manipulate it\n# drop ID because uneeded for prediction\npreprocess_df = pd.concat((train_df, test_df)).reset_index(drop=True)\npreprocess_df.drop(['SalePrice'], axis=1, inplace=True)\npreprocess_df.drop(['Id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da50dbdc4f19861e8c6325c813129427e71c4921"},"cell_type":"code","source":"for feature in possible_features:\n    print(feature)\n    print(preprocess_df[feature].value_counts())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"376a07a0b3dc78dd0db6e5e2966522345c21e6b9"},"cell_type":"code","source":"preprocess_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88de0a85cf6b2c32335ddcbf4984a919d12928d7"},"cell_type":"code","source":"preprocess_df.hist(bins=50,figsize=(40,30))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"433855d318aa1fe17f2b5549c512669563ac1878"},"cell_type":"code","source":"preprocess_df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c58496cc5c87f28a23d4f170ef97e5e9ced98e93"},"cell_type":"code","source":"# separate dataframe by types for preprocessing\ng = preprocess_df.columns.to_series().groupby(preprocess_df.dtypes).groups\n\n# separate into groups by object, convert to a dictionary\ng_dict = {k.name: v for k, v in g.items()}\nprint(g_dict['object'])\nprint('\\n')\n\n# use dictionary to get unique values for each categorical feature\nobject_features = []\ntotal_features = 0\nfor item in g_dict['object']:\n    print(item)\n    object_features.append(item)\n    print(preprocess_df[item].unique())\n    list_length = len(preprocess_df[item].unique())\n    print(list_length)\n    total_features += list_length\n    \n    print('\\n')\n\n# for future, non object features\nnon_object = list(set(preprocess_df.columns.tolist()) - set(object_features))\nprint(non_object)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"474e7251c8f961f19ce535b8db829b2e93bbefb7"},"cell_type":"code","source":"# data cleaning non object features\n# impute the median value for non object features\nimputer = Imputer(strategy=\"median\")\n\nimputer.fit(preprocess_df[non_object])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60ab215b7b67c14cb6f1a03e6277d602ad22d4b2"},"cell_type":"code","source":"# find median\nX = imputer.transform(preprocess_df[non_object])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b8392ab2c1fb1792294640a4c544204997a94f6"},"cell_type":"code","source":"# set median\npreprocess_df[non_object] = pd.DataFrame(X, columns=non_object)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4b42509b74400294e60a5a80e1042eef4668298"},"cell_type":"code","source":"preprocess_df[non_object].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f9f5c629e7a04c070f3be1f2e97f4388642271c"},"cell_type":"code","source":"# because lazy, fill missing categorical features with most frequent value\nfor of in object_features:\n    frequent_value = preprocess_df[of].value_counts().index[0]\n    preprocess_df[of] = preprocess_df[of].fillna(frequent_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7f43adf599fe9702e30d7772b8f3145309fceff"},"cell_type":"code","source":"preprocess_df[object_features].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"accead60814d48c33c1e82ec8c2aeeb9e08b07b7"},"cell_type":"code","source":"# one hot encoding of categorical values\npre_process_1hot = pd.get_dummies(preprocess_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a380b2a1466fb175ea382511c9e670eb5ea9bdae"},"cell_type":"code","source":"pre_process_1hot.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"1fd73319ede0ef43791912cc740bb58b25fde58f"},"cell_type":"code","source":"# test to see if this is enough feature engineering\n#split data sets again\nprocessed_train = pre_process_1hot[:index_train]\nprocessed_test = pre_process_1hot[index_train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6df0e4048d37f58929f5bb50cf099b0ff3722dc9"},"cell_type":"code","source":"processed_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"219d125a7b5eb002f041a6049c9c4ab6e71b16c2"},"cell_type":"code","source":"# normalize target\n\nnormalize_target = target_df.copy()\n\nmin_max_target = (normalize_target - normalize_target.min())/(normalize_target.max()-normalize_target.min())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac1d22e70bd8172111d4367b86736734ec54270a"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebbff6f19d489f53402fae10e7c9693acbf5b179"},"cell_type":"code","source":"def display_scores(scores):\n    print(\"Scores: \", scores)\n    print(\"Mean: \", scores.mean())\n    print(\"Standard Deviation: \", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e160deabdbb856e90cd6a0a1bd98a974ba818f6"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor()\ngbr.fit(processed_train,target_df)\ngbr_scores = cross_val_score(gbr,processed_train,target_df,scoring='neg_mean_squared_error',cv=10)\n\ngbr_rmse=np.sqrt(-gbr_scores)\n\ndisplay_scores(gbr_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e7f78d2e6c8453cb5dc24d25aab7aa2ac348300"},"cell_type":"code","source":"# parameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'learning_rate':[.01,.05,.1,.5,.75],'n_estimators': [30,50,70,90],'max_features':[10,14,18,20]}\n]\n\ngrid_search = GridSearchCV(gbr, param_grid, cv=5, \n                           scoring='neg_mean_squared_error')\n\ngrid_search.fit(processed_train,target_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5294a05ea54c6f814b477c8bad1dbe1088a2371"},"cell_type":"code","source":"param_dict = {'n_estimators': [30,50,70,90],'max_features':[10,14,18,20], 'max_depth':[2,3,4,5,6]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30a43579287d829fdf4574d1fb1163ceb077d325"},"cell_type":"code","source":"best_params = grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a73a026b5af345dd54f906ec49573662adfb9f06"},"cell_type":"code","source":"# some initial values\nscore_dict = {}\ni = 0\nlast_score = 100000\n\nparam_dict = {'n_estimators': [30,50,70,90],'max_features':[10,14,18,20], 'max_depth':[2,3,4,5,6]}\nnew_param_grid = [param_dict]\n\n\n# run once to get values\ngrid_search = GridSearchCV(gbr, new_param_grid, cv=5, \n                           scoring='neg_mean_squared_error')\ngrid_search.fit(processed_train,target_df)\n\nbest_params = grid_search.best_params_\nbest_score = np.sqrt(-grid_search.best_score_)\nlast_param_grid = [param_dict]\n\nprint('Iteration: ', i)\nprint('Current Score: ', best_score)\nprint('Current Settings: ', best_params)\n\n# iterate\nwhile i < 20:\n    if best_score < last_score:\n        # param_dict['learning_rate'] = [max(.1,best_params['learning_rate']-.01),best_params['learning_rate'],best_params['learning_rate']+.01,best_params['learning_rate']+.05]\n        param_dict['n_estimators'] = [max(1,best_params['n_estimators']-1),best_params['n_estimators'],best_params['n_estimators']+1,best_params['n_estimators']+5]\n        param_dict['max_features'] = [max(1,best_params['max_features']-1),best_params['max_features'],best_params['max_features']+1,best_params['max_features']+5]\n        param_dict['max_depth'] = [max(1,best_params['max_depth']-1),best_params['max_depth'],best_params['max_depth']+1,best_params['max_depth']+5]\n        new_param_grid = [param_dict]\n        i += 1\n    else:\n        # param_dict['learning_rate'] = [max(.1,best_params['learning_rate']-.05),best_params['learning_rate'],best_params['learning_rate'],best_params['learning_rate']+.01]\n        param_dict['n_estimators'] = [max(1,best_params['n_estimators']-5),max(1,best_params['n_estimators']-1),best_params['n_estimators'],best_params['n_estimators']+1]\n        param_dict['max_features'] = [max(1,best_params['max_features']-5),max(1,best_params['max_features']-1),best_params['max_features'],best_params['max_features']+1]\n        param_dict['max_depth'] = [max(1,best_params['max_depth']-5),max(1,best_params['max_depth']-1),best_params['max_depth'],best_params['max_depth']+1]\n        new_param_grid = [param_dict]\n        i += 1\n\n    grid_search = GridSearchCV(gbr, new_param_grid, cv=5, \n                           scoring='neg_mean_squared_error')\n    grid_search.fit(processed_train,target_df)\n    \n    best_params = grid_search.best_params_\n    best_score = np.sqrt(-grid_search.best_score_)\n    \n    score_dict[best_score] = best_params\n    \n    print('Iteration: ', i)\n    print('Current Score: ', best_score)\n    print('Current Settings: ', best_params)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7add225daa87555cc88020b086d0ce04a506cb0e","scrolled":true},"cell_type":"code","source":"bestest_params = score_dict[min(score_dict.keys())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7180065bce4998519ecd1c54a30441a08c945654"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ba33b0077eb57d822b07b512c7ec23702cd8502"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"b3e03a1855a27ec6dc1ff1073f6e6e20ce2a857a"},"cell_type":"code","source":"gbr = GradientBoostingRegressor(max_depth=bestest_params['max_depth'],max_features=bestest_params['max_features'],n_estimators=bestest_params['n_estimators'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"47995f3a0163be61e31ab7823d107823189e53f9"},"cell_type":"code","source":"gbr.fit(processed_train,target_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3274ad3af263510867794ac89d9e1a8a1ef2c4a8"},"cell_type":"code","source":"gbr_predict = gbr.predict(processed_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d31b4dc31245105bc235eec5a4139faa42823b1"},"cell_type":"code","source":"submission = test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c91d8e6bc7b6fd16d5f3ba35c9a4aabc103d00c6"},"cell_type":"code","source":"submission['SalePrice'] = gbr_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d860bb814f7d958357539cdea7bedc38cab67c1d"},"cell_type":"code","source":"submission[['Id','SalePrice']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c217f61a3529a2534d800546c778a8925f097bd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}